\begin{thebibliography}{}
    \bibitem{w2v} Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean. “Efficient Estimation of Word Representations in Vector Space.” (2013).
    \bibitem{ft} Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov. “Enriching Word Vectors with Subword Information.” (2017).
    \bibitem{transformer} Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. “Attention Is All You Need.” (2017).
    \bibitem{bert} Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. “BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.” (2018).
    \bibitem{bpe} Philip Gage. “A New Algorithm for Data Compression.” (1994).
    \bibitem{med_survey} Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen A.W.M. van der Laak, Bram van Ginneken, Clara I. Sanchez. “A Survey on Deep Learning in Medical Image Analysis.” (2017).
    \bibitem{word_piece} Mike Schuster, Kaisuke Nakajima, "Japanese and Korean voice search." (2012)
    \bibitem{pe} Kazemnejad Amirhossein, "Transformer Architecture: The Positional Encoding." (2019)
    \bibitem{RNN_survey} Zachary C. Lipton, John Berkowitz, Charles Elkan "A Critical Review of Recurrent Neural Networks for Sequence Learning." (2015)
    \bibitem{LSTM} Sepp Hochreiter, J¨urgen Schmidhuber "Long short-term memory." (1997)
    \bibitem{Attention} Dzmitry Bahdanau, Kyung Hyun Cho, Yoshua Bengio "Neural Machine Translation by Jointly Learning to Align and Translate." (2014)
    \bibitem{NBME} https://www.kaggle.com/competitions/nbme-score-clinical-patient-notes (2022)
    \bibitem{model_expl} Scott M. Lundberg, Su-In Lee "A Unified Approach to Interpreting Model Predictions." (2017)
    \bibitem{shap_models} Mukund Sundararajan, Amir Najmi "The Many Shapley Values for Model Explanation." (2020)
    \bibitem{bert_shap} Blaz Skrlj, Shane Sheehan, Nika Erzen, Marko Robnik-Sikonja, Saturnino Luz, Senja Pollak "BERT meets Shapley: Extending SHAP Explanations to Transformer-based Classifiers." (2020)
\end{thebibliography}
% добавляем библиографию в содержание
\addcontentsline{toc}{section}{Список используемой литературы}
